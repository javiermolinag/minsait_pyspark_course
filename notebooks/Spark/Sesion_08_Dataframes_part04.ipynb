{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a491a835-7add-4f73-9cb9-f5c7689e555b",
   "metadata": {},
   "source": [
    "- #### Transformaciones\n",
    "    - ##### join\n",
    "    - ##### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956efa9-7ad7-4446-94c1-e2c207fcf290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "\n",
    "val spark = SparkSession.builder\n",
    "        .appName(\"sesion_1\")\n",
    "        .master(\"local[*]\")\n",
    "        .getOrCreate()\n",
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325c819-1f91-47a1-8e40-3041b716f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.uiWebUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b206961-7a2a-433f-895e-26834d4cf524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCsv(path: String): DataFrame = {\n",
    "    spark.read\n",
    "        .option(\"header\",\"true\")\n",
    "        .option(\"delimiter\",\",\")\n",
    "        .option(\"inferSchema\",\"false\")\n",
    "        .csv(path)\n",
    "}\n",
    "\n",
    "val BasePath = \"../../resources/data/csv/\"\n",
    "val clientsDf = readCsv(BasePath + \"clients.csv\")\n",
    "val contractsDf = readCsv(BasePath + \"contracts.csv\")\n",
    "val productsDf = readCsv(BasePath + \"products.csv\")\n",
    "\n",
    "clientsDf.show()\n",
    "contractsDf.show()\n",
    "productsDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7305768-af30-40e4-bf19-3df8a2f55685",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Joins\n",
    "\n",
    "// inner -> Mantiene información de ambas tablas (columnas) para los registros (filas) coincidentes\n",
    "// outer -> Mantiene información de ambas tablas (columnas y filas) para los registros coincidentes y no-coincidentes\n",
    "// left -> Mantiene columnas de ambas tablas y filas únicamente de la tabla izquierda, elimina filas no coincidentes de la tabla derecha\n",
    "// right -> Mantiene columnas de ambas tablas y filas únicamente de la tabla derecha, elimina filas no coincidentes de la tabla izquierda\n",
    "// left_semi -> Mantiene filas y columnas únicamente de la tabla izquierda para los registros coincidentes\n",
    "// left_anti -> Mantiene filas y columnas únicamente de la tabla izquierda para los registros no-coincidentes\n",
    "\n",
    "// cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cb50e-4d3d-4bf0-8687-4b2db989b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{functions => f}\n",
    "\n",
    "val clientstmpdf = clientsDf.filter((f.col(\"edad\") >= 40) && (f.col(\"edad\") <= 50))\n",
    "val contractsTmpDf = contractsDf.filter(f.col(\"activo\") === \"false\")\n",
    "    .withColumnRenamed(\"cod_titular\", \"cod_client\")\n",
    "\n",
    "clientstmpdf.show()\n",
    "contractsTmpDf.show()\n",
    "\n",
    "// clientstmpdf.crossJoin(contractsTmpDf).show() // WARNING\n",
    "\n",
    "val JoinType = \"full\"   // inner, outer, left, right, left_semi, left_anti\n",
    "\n",
    "val joinDf = clientstmpdf.join(contractsTmpDf, Seq(\"cod_client\"), JoinType)\n",
    "joinDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade859d-d327-48fb-a7fd-ee69f7b280ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "// UDF - User Defined Function - WARNING\n",
    "import org.apache.spark.sql.{types => t}\n",
    "import scala.util.Try\n",
    "\n",
    "val upperCaseFunction: String => Option[String] = value => Try(value.toUpperCase()).toOption\n",
    "\n",
    "val len_concat: (String, String) => Int = (item_1, item_2) => {\n",
    "    val EmptyString = \"\"\n",
    "    val str1 = if (Option(item_1).isEmpty) EmptyString else item_1\n",
    "    val str2 = if (Option(item_2).isEmpty) EmptyString else item_2\n",
    "    \n",
    "    str1.concat(str2).size\n",
    "}\n",
    "\n",
    "val upperUdf = f.udf(upperCaseFunction)\n",
    "\n",
    "val lenConcatUdf = f.udf(len_concat)\n",
    "\n",
    "joinDf.select(\n",
    "    joinDf.columns.map(f.col) :+\n",
    "    upperUdf(f.col(\"nombre\")).alias(\"nombre_mayus\") :+\n",
    "    lenConcatUdf(f.col(\"nombre\"), f.col(\"provincia\")).alias(\"len_concat\") :_*\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3b35a-ce9f-4acd-b8a1-39f3507e8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq(Row(1,null),\n",
    "               Row(2,Double.NaN),\n",
    "               Row(3,3.2),\n",
    "               Row(4,Double.NaN))\n",
    "\n",
    "val schema = t.StructType(Seq(\n",
    "    t.StructField(\"id\", t.IntegerType),\n",
    "    t.StructField(\"number\", t.DoubleType)))\n",
    "\n",
    "val tmpDf = spark.createDataFrame(sc.parallelize(data), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab3e63-f92d-4849-a184-44296988c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpDf.show()\n",
    "// Scala no puede revisar si un AnyVal es null, lo único que podemos hacer es procesar el NULL desde el api de SparkSQL\n",
    "\n",
    "val isNullOrNan: Double => Boolean = value => if (value.isNaN) true else false\n",
    "    \n",
    "val isNullOrNanUdf = f.udf(isNullOrNan)\n",
    "\n",
    "tmpDf.select(\n",
    "    tmpDf.columns.map(f.col) :+\n",
    "    isNullOrNanUdf(f.col(\"number\")).alias(\"null_nan\") :_*\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2b5b9-8eb1-46cb-8dc1-58a66fb82282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
