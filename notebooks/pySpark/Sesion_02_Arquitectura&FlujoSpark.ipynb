{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df4e982-562f-4ee6-a7e9-4fecda27c496",
   "metadata": {},
   "source": [
    "- #### Arquitectura\n",
    "- #### Flujo de ejecución\n",
    "- #### Transformaciónes vs Acciones\n",
    "- #### Optimizador Catalist\n",
    "- #### DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471db50d-a05e-44c9-8541-af21d996d01b",
   "metadata": {},
   "source": [
    "## Spark architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503a76ae-8afb-4223-b18b-ead631a2240b",
   "metadata": {},
   "source": [
    "![Spark_architecture](../../resources/img/SPARK.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da20d3-254a-4467-8fc0-a9d4850bfa05",
   "metadata": {},
   "source": [
    "#### Driver program\n",
    "The process running the main() function of the application and creating the SparkContext. **Starts the application and sends work to the executors.**\n",
    "\n",
    "#### Cluster manager\n",
    "An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN, Kubernetes)\n",
    "\n",
    "   - **Standalone** – a simple cluster manager included with Spark that makes it easy to set up a cluster.\n",
    "   - **Apache Mesos** – a general cluster manager that can also run Hadoop MapReduce and service applications. (Deprecated)\n",
    "   - **Hadoop YARN** – the resource manager in Hadoop 2 and 3.\n",
    "   - **Kubernetes** – an open-source system for automating deployment, scaling, and management of containerized applications.\n",
    "\n",
    "#### Worker node\t\n",
    "Any node that can run application code in the cluster\n",
    "\n",
    "#### Executor\n",
    "A process launched for an application on a worker node, that runs tasks and keeps data in memory or disk storage across them. Each application has its own executors. **Launched in JVM containers with their own memory and CPU resources**\n",
    "\n",
    "#### Deploy mode\t\n",
    "Distinguishes where the driver process runs. \n",
    "\n",
    "- **Cluster mode** - the Spark driver is launched on a worker node\n",
    "- **Client mode** - the Spark driver is on the client machine\n",
    "- **Local mode** - the entire application runs on the same machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490339b3-5622-4c8b-a3be-804726a1762e",
   "metadata": {},
   "source": [
    "## Spark app execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639bb527-b442-4b2f-beda-5ed1be6e446b",
   "metadata": {},
   "source": [
    "- An action triggers a **Job**\n",
    "  \n",
    "- A job is split into **stages**\n",
    "    - each stage is dependent on the stage before it\n",
    "    - a stage must fully complete before the next stage can start\n",
    "    - for performance (usually) minimize the number of stages\n",
    "\n",
    "- A stage has **tasks**\n",
    "    - task = smallest unit of work\n",
    "    - tasks are run by executors\n",
    "\n",
    "- **An RDD/DF/DS has partitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e4b79-5329-4734-b919-708b64e069f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"sesion_2\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62276910-79b2-48a8-8566-d93c9d126b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stage 1\n",
    "employees = sc.textFile(\"../../resources/data/csv/employees.csv\")\n",
    "empTokens = employees.map(lambda line: line.split(\",\"))\n",
    "empDetails = empTokens.map(lambda tokens: (tokens[4], float(tokens[7])))\n",
    "## Stage 2\n",
    "empGroups = empDetails.groupByKey(2)\n",
    "avgSalaries = empGroups.mapValues(lambda salaries: sum(list(salaries)) / len(salaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864d994-8707-4475-b57a-f72681a4928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgSalaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352820d2-4e6f-4bd2-8819-3fd1380de697",
   "metadata": {},
   "source": [
    "#### **TRANSFORMATIONS ARE LAZY - SO ARE EXECUTED WHEN AN ACTION IS TRIGGERED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307b490-6f59-4adc-ba18-f92ea12a0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "avgSalaries.foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431c7b9-9297-4bd5-872c-cd30df0f3490",
   "metadata": {},
   "source": [
    "![Image](../../resources/img/Stages_tasks.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c791c-6393-473d-9520-dd7e30c42fb8",
   "metadata": {},
   "source": [
    "#### Shuffle\n",
    "\n",
    "- Exchange of data between executors\n",
    "- happens in between stages\n",
    "- must complete before next stage starts\n",
    "\n",
    "- Expensive because of\n",
    "    - transferring data\n",
    "    - serialization/deserialization\n",
    "    - loading new data from shufflefiles\n",
    "- Shuffles are performance bottlenecks because\n",
    "    - exchanging data takes time\n",
    "    - they need to be fully completed before next computations start\n",
    "- Shuffles limit parallelization\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0d7fd9-5093-413f-8d0a-c2e2e79f5daf",
   "metadata": {},
   "source": [
    "#### Concepts Relationship\n",
    "App decomposition\n",
    "- 1 job = 1 or more stages\n",
    "- 1 stage = 1 or more tasks\n",
    "\n",
    "Tasks & executors\n",
    "- 1 task is run by 1 executor\n",
    "- each executor can run 0 or more tasks\n",
    "\n",
    "Partitions and tasks\n",
    "- processing 1 partition = one task\n",
    "\n",
    "Partitions & executors\n",
    "- 1 partition stays on 1 executor\n",
    "- each executor can load 0 or more partitions in memory or disk\n",
    "\n",
    "Executors & nodes\n",
    "- 1 executor = 1 JVM on 1 physical node\n",
    "- each physical node can have 0 or more executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ead6f94-a013-4f2c-b13f-9fd28b92b6fc",
   "metadata": {},
   "source": [
    "## Transformations vs Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e816447-19b5-49eb-8596-81f48ca6c911",
   "metadata": {},
   "source": [
    "- Transformations describe how new DFs are obtained\n",
    "- Actions start executing Spark code\n",
    "- Transformations return RDDs/DFs\n",
    "- Actions return something else e.g. Unit, number, etc.\n",
    "\n",
    "#### Narrow Transformations\n",
    "\n",
    "- given a parent partition, a single child partition depends on it\n",
    "- fast to compute\n",
    "- examples: union, map, flatMap, select, filter\n",
    "\n",
    "#### Wide transformations\n",
    "\n",
    "- given a parent partition, more than one child partitions depend on it\n",
    "- involve a shuffle = data transfer between Spark executors\n",
    "- are costly to compute\n",
    "- examples: joining, grouping, sorting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58568431-33d1-4876-bba3-a42832920bac",
   "metadata": {},
   "source": [
    "## Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd24766-b833-4239-9ea4-ebb91c53b22a",
   "metadata": {},
   "source": [
    "![Image](../../resources/img/Catalist_Optimizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee5d348-df9e-4cc2-87e4-dc609c4ec89f",
   "metadata": {},
   "source": [
    "1. Catalyst Optimizer and Tungsten Execution Engine was introduced in Spark 1.x\n",
    "2. Cost-Based Optimizer was introduced in Spark 2.x\n",
    "3. Adaptive Query Execution now got introduced in Spark 3.x\n",
    "\n",
    "    - To disable the Adaptive Query Execution -> spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
    "\n",
    "Only works with DF and DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f8997a-da68-4f75-8844-22c9ffcd5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = spark.range(1, 100000000)\n",
    "ds2 = spark.range(1, 100000000, 2)\n",
    "ds3 = ds1.repartition(7)\n",
    "ds4 = ds2.repartition(9)\n",
    "ds5 = ds3.selectExpr(\"id * 3 as id\")\n",
    "joined = ds5.join(ds4, \"id\")\n",
    "sum = joined.selectExpr(\"sum(id)\")\n",
    "sum.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9e1e8d-0af3-422f-8e11-83980de000aa",
   "metadata": {},
   "source": [
    "## DAG\n",
    "\n",
    "DAG = Directed Acyclic Graph = visual representation of Spark jobs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c540e-2ae2-401b-9ff4-31e9221a95a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark\n",
    "# http://localhost:4040/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1ecfda-d392-4e89-a663-0059b42dc47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum.count() # Call an action to trigger transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294ef15-dbd0-4724-a0b1-1538353e1d1e",
   "metadata": {},
   "source": [
    "![Image](../../resources/img/DAG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7a943-44fa-4adf-aed2-8315f19cc3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
