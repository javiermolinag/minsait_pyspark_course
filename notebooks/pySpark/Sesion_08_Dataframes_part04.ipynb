{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a491a835-7add-4f73-9cb9-f5c7689e555b",
   "metadata": {},
   "source": [
    "- #### Transformaciones\n",
    "    - ##### join\n",
    "    - ##### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6956efa9-7ad7-4446-94c1-e2c207fcf290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"sesion_1\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b206961-7a2a-433f-895e-26834d4cf524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "    return spark.read\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .option(\"delimiter\",\",\")\\\n",
    "        .option(\"inferSchema\",\"false\")\\\n",
    "        .csv(path)\n",
    "\n",
    "base_path = \"../../resources/data/csv/\"\n",
    "clients_df = read_csv(base_path + \"clients.csv\")\n",
    "contracts_df = read_csv(base_path + \"contracts.csv\")\n",
    "products_df = read_csv(base_path + \"products.csv\")\n",
    "\n",
    "clients_df.show()\n",
    "contracts_df.show()\n",
    "products_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7305768-af30-40e4-bf19-3df8a2f55685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joins\n",
    "\n",
    "# inner -> Mantiene información de ambas tablas (columnas) para los registros (filas) coincidentes\n",
    "# outer -> Mantiene información de ambas tablas (columnas y filas) para los registros coincidentes y no-coincidentes\n",
    "# left -> Mantiene columnas de ambas tablas y filas únicamente de la tabla izquierda, elimina filas no coincidentes de la tabla derecha\n",
    "# right -> Mantiene columnas de ambas tablas y filas únicamente de la tabla derecha, elimina filas no coincidentes de la tabla izquierda\n",
    "# left_semi -> Mantiene filas y columnas únicamente de la tabla izquierda para los registros coincidentes\n",
    "# left_anti -> Mantiene filas y columnas únicamente de la tabla izquierda para los registros no-coincidentes\n",
    "\n",
    "# cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18cb50e-4d3d-4bf0-8687-4b2db989b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "clients_tmp_df = clients_df.filter((f.col(\"edad\") >= 40) & (f.col(\"edad\") <= 50))\n",
    "contracts_tmp_df = contracts_df.filter(f.col(\"activo\") == \"false\")\\\n",
    "    .withColumnRenamed(\"cod_titular\", \"cod_client\")\n",
    "\n",
    "clients_tmp_df.show()\n",
    "contracts_tmp_df.show()\n",
    "\n",
    "# clients_tmp_df.crossJoin(contracts_tmp_df).show() ## WARNING\n",
    "\n",
    "typw_join = \"full\"   # inner, outer, left, right, left_semi, left_anti\n",
    "\n",
    "join_df = clients_tmp_df.join(contracts_tmp_df, [\"cod_client\"], typw_join)\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ade859d-d327-48fb-a7fd-ee69f7b280ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF - User Defined Function - WARNING\n",
    "import pyspark.sql.types as t\n",
    "\n",
    "def upperCase(value):\n",
    "    if value is None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return value.upper()\n",
    "\n",
    "def len_concat(item_1, item_2):\n",
    "    if item_1 is None:\n",
    "        item_1 = \"\"\n",
    "    if item_2 is None:\n",
    "        item_2 = \"\"\n",
    "    return len(item_1 + item_2)\n",
    "\n",
    "upper_udf = f.udf(upperCase, t.StringType())\n",
    "\n",
    "len_concat_udf = f.udf(len_concat, t.LongType())\n",
    "\n",
    "join_df.select(\n",
    "    *join_df.columns,\n",
    "    upper_udf(f.col(\"nombre\")).alias(\"nombre_mayus\"),\n",
    "    len_concat_udf(f.col(\"nombre\"), f.col(\"provincia\")).alias(\"len_concat\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3b35a-ce9f-4acd-b8a1-39f3507e8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "tmp_df = spark.createDataFrame([\n",
    "    Row(1,None),\n",
    "    Row(2,float(\"nan\")),\n",
    "    Row(3,3.2),\n",
    "    Row(4,float(\"nan\"))], [\"id\", \"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aab3e63-f92d-4849-a184-44296988c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df.show()\n",
    "\n",
    "def is_null_or_nan(value):\n",
    "    if (value!=value) | (value is None):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "is_null_or_nan_udf = f.udf(is_null_or_nan, t.BooleanType())\n",
    "\n",
    "tmp_df.select(\n",
    "    *tmp_df.columns,\n",
    "    is_null_or_nan_udf(f.col(\"number\")).alias(\"null_nan\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2b5b9-8eb1-46cb-8dc1-58a66fb82282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
